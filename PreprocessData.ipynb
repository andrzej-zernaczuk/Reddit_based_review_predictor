{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import tensorflow_text as tf_text\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense, InputLayer, GlobalMaxPool1D, Dropout, Conv1D, MaxPool1D, Flatten, Embedding, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.activations import relu, sigmoid\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORD2VEC \n",
    "W2V_SIZE = 300\n",
    "W2V_WINDOW = 7\n",
    "W2V_EPOCH = 32\n",
    "W2V_MIN_COUNT = 10\n",
    "\n",
    "# KERAS \n",
    "SEQUENCE_LENGTH = 300\n",
    "EPOCHS = 8\n",
    "BATCH_SIZE = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_data_prep():\n",
    "    data = pd.read_csv('Data/twitter_data.csv', \n",
    "                      encoding='latin-1', names=['sentiment', 'id', 'date', 'flag', 'user',\n",
    "                                                'text'])\n",
    "    data['sentiment'].replace(4, 1, inplace=True)\n",
    "    data = data.sample(frac=1).reset_index(drop=True).copy()\n",
    "    data['text'] = data['text'].str.lower()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_nicknames(row):\n",
    "    # Prepare list of words\n",
    "    words = row.split()\n",
    "    # Remove nicknames\n",
    "    for word in words:\n",
    "        if word[0] == '@':\n",
    "            words.remove(word)\n",
    "    # Return string \n",
    "    return ' '.join(word for word in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split(df, size):\n",
    "    # Split dataset into smaller one \n",
    "    col_list = list(df.columns)\n",
    "    # Drop target column name\n",
    "    col_list.pop(0)\n",
    "    x_train, x_valid = train_test_split(\n",
    "    df, random_state=1, stratify=df['sentiment'], test_size=size)\n",
    "    # Prepare new indexes \n",
    "    x_valid.reset_index(drop=True, inplace=True)\n",
    "    return x_valid \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_space(text):\n",
    "    # Replace new rows with space \n",
    "    text = text.replace('\\n', \" \").replace(\"\\r\", \" \")\n",
    "    # Create list of all not needed chars \n",
    "    punc_list = '!\"@#$%^&*()+_-.<>?/:;[]{}|\\~'\n",
    "    # Make transformation with dict that contains punc_list chars\n",
    "    t = str.maketrans(dict.fromkeys(punc_list, \" \"))\n",
    "    # Apply transformation\n",
    "    text = text.translate(t)\n",
    "    # Replace single quote with empty char\n",
    "    t = text.maketrans(dict.fromkeys(\"'`\"))\n",
    "    text.translate(t)\n",
    "    \n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text):\n",
    "    # Prepare set of stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Not not found on win10, returns Key Error\n",
    "#     stop_words.remove('Not')\n",
    "\n",
    "    # Remove stopwords from the text\n",
    "    filtered_text = [word for word in text.split() if not word in stop_words]\n",
    "    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data, num_words, num_words_pad): \n",
    "    data = data.copy()\n",
    "    # Apply replace func that replace chars with spaces\n",
    "    data['text'] = data['text'].apply(lambda x: replace_with_space(x)).copy()\n",
    "    # Apply func that removes stop words\n",
    "    data['text'] = data['text'].apply(lambda x: remove_stop_words(x))\n",
    "    \n",
    "    \n",
    "    w2v_model = gensim.models.word2vec.Word2Vec(vector_size=W2V_SIZE, \n",
    "                                            window=W2V_WINDOW, \n",
    "                                            min_count=W2V_MIN_COUNT, \n",
    "                                            workers=8)\n",
    "    \n",
    "    documents = [_text.split() for _text in df_train.text] \n",
    "    w2v_model.build_vocab(documents)\n",
    "\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    tok = tf.keras.preprocessing.text.Tokenizer(num_words=num_words)\n",
    "    # Updates internal vocabulary based on a list of texts \n",
    "    tok.fit_on_texts(list(data['text']))\n",
    "    # Transforms each text in texts to a sequence of integers.\n",
    "    seq = tok.texts_to_sequences(list(data['text']))\n",
    "    # Pad sequences to make them same lenght \n",
    "    tf_ready = tf.keras.preprocessing.sequence.pad_sequences(seq)\n",
    "    \n",
    "    return tf_ready, tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = first_data_prep()\n",
    "# Apply replace func that replace chars with spaces\n",
    "data['text'] = data['text'].apply(lambda x: replace_with_space(x)).copy()\n",
    "# Apply func that removes stop words\n",
    "data['text'] = data['text'].apply(lambda x: remove_stop_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 27.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tok = tf.keras.preprocessing.text.Tokenizer()\n",
    "# Updates internal vocabulary based on a list of texts \n",
    "tok.fit_on_texts(list(data['text']))\n",
    "# Transforms each text in texts to a sequence of integers.\n",
    "seq = tok.texts_to_sequences(list(data['text']))\n",
    "# Pad sequences to make them same lenght \n",
    "tf_ready = tf.keras.preprocessing.sequence.pad_sequences(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "w2v_model = gensim.models.word2vec.Word2Vec(vector_size=W2V_SIZE, \n",
    "                                            window=W2V_WINDOW, \n",
    "                                            min_count=W2V_MIN_COUNT, \n",
    "                                            workers=8)\n",
    "    \n",
    "documents = data.text\n",
    "w2v_model.build_vocab(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 739298\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "words = w2v_model.wv.index_to_key\n",
    "vocab_size = len(tok.word_index) + 1\n",
    "print(\"Vocab size\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 7s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(355796583, 415799456)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "w2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, W2V_SIZE))\n",
    "for word, i in tok.word_index.items():\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[i] = w2v_model.wv[word]\n",
    "embedding_layer = Embedding(vocab_size, W2V_SIZE,\n",
    "                            weights=[embedding_matrix], input_length=SEQUENCE_LENGTH, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_df = pd.DataFrame(tf_ready)\n",
    "tf_df['sentiment'] = data['sentiment']\n",
    "# tf_df.to_csv('tokenized_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-4e8f90234d2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                  \u001b[0mkernel_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'lecun_normal'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                  \u001b[0mkernel_regularizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mL2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m                 ):\n\u001b[0;32m     10\u001b[0m     \"\"\" Layers argument shape:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "def configure_cnn(data,\n",
    "                embedding,\n",
    "                layers=None, \n",
    "                dropout_rate=0,\n",
    "                optimizer='Adam',\n",
    "                loss='binary_crossentropy',\n",
    "                 kernel_initializer='lecun_normal',\n",
    "                 kernel_regularizer=tf.keras.regularizers.L2(0.01)\n",
    "                ):\n",
    "    \"\"\" Layers argument shape:\n",
    "    [[number of nodes, activate function], \n",
    "    [number of nodes, activate function],\n",
    "    ...]\n",
    "    \n",
    "    \"\"\"\n",
    "    input_len = data.shape[1] - 1 \n",
    "    model = Sequential()\n",
    "    model.add(embedding)\n",
    "    model.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "    model.add(MaxPool1D(3))\n",
    "    model.add(Conv1D(64, 3, activation='relu'))\n",
    "    model.add(MaxPool1D(pool_size=3))\n",
    "    model.add(Conv1D(128, 3, activation='relu'))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    if dropout_rate > 0:\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    if layers != None:\n",
    "        for node in layers[1:]:\n",
    "            model.add(Dense(node[0], activation=node[1], kernel_initializer=kernel_initializer, \n",
    "                            kernel_regularizer=kernel_regularizer))\n",
    "    model.add(Dense(10, activation='sigmoid'))\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_initializer=kernel_initializer, \n",
    "                    kernel_regularizer=kernel_regularizer))\n",
    "    \n",
    "    model.compile(loss=loss, \n",
    "                 optimizer=optimizer,\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_rnn(data,\n",
    "                embedding,\n",
    "                layers=None, \n",
    "                dropout_rate=0,\n",
    "                optimizer='Adam',\n",
    "                loss='binary_crossentropy',\n",
    "                 kernel_initializer='lecun_normal',\n",
    "                 kernel_regularizer=tf.keras.regularizers.L2(0.01)\n",
    "                ):\n",
    "    \"\"\" Layers argument shape:\n",
    "    [[number of nodes, activate function], \n",
    "    [number of nodes, activate function],\n",
    "    ...]\n",
    "    \n",
    "    \"\"\"\n",
    "    input_len = data.shape[1] - 1 \n",
    "    model = Sequential()\n",
    "    model.add(embedding)\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))    \n",
    "\n",
    "    if layers != None:\n",
    "        for node in layers[1:]:\n",
    "            model.add(Dense(node[0], activation=node[1], kernel_initializer=kernel_initializer, \n",
    "                            kernel_regularizer=kernel_regularizer))\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_initializer=kernel_initializer, \n",
    "                    kernel_regularizer=kernel_regularizer))\n",
    "    \n",
    "    model.compile(loss=loss, \n",
    "                 optimizer=optimizer,\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'configure_cnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0d1283cf5a14>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfigure_cnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# model_rnn = configure_rnn(tf_df, embedding_layer)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'configure_cnn' is not defined"
     ]
    }
   ],
   "source": [
    "model = configure_cnn(tf_df, embedding_layer)\n",
    "# model_rnn = configure_rnn(tf_df, embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tf_df.columns.tolist()\n",
    "features.remove('sentiment')\n",
    "X_train, X_test, y_train, y_test = train_test_split(tf_df[features], tf_df['sentiment'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 8088951385268926044\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 300, 300)          44304600  \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 300, 32)           28832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 100, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 98, 64)            6208      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 32, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 30, 128)           24704     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 44,365,645\n",
      "Trainable params: 61,045\n",
      "Non-trainable params: 44,304,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_rnn.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 300) for input KerasTensor(type_spec=TensorSpec(shape=(None, 300), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 49).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 300) for input KerasTensor(type_spec=TensorSpec(shape=(None, 300), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 49).\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.7205 - accuracy: 0.5246WARNING:tensorflow:Model was constructed with shape (None, 300) for input KerasTensor(type_spec=TensorSpec(shape=(None, 300), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 49).\n",
      "105/105 [==============================] - 14s 128ms/step - loss: 0.7202 - accuracy: 0.5248 - val_loss: 0.6702 - val_accuracy: 0.5859\n",
      "Epoch 2/8\n",
      "105/105 [==============================] - 16s 149ms/step - loss: 0.6631 - accuracy: 0.5951 - val_loss: 0.6654 - val_accuracy: 0.5868\n",
      "Epoch 3/8\n",
      "105/105 [==============================] - 17s 166ms/step - loss: 0.6525 - accuracy: 0.6064 - val_loss: 0.6645 - val_accuracy: 0.5761\n",
      "Epoch 4/8\n",
      "105/105 [==============================] - 18s 167ms/step - loss: 0.6463 - accuracy: 0.6084 - val_loss: 0.6653 - val_accuracy: 0.5850\n",
      "Epoch 5/8\n",
      "105/105 [==============================] - 17s 165ms/step - loss: 0.6360 - accuracy: 0.6207 - val_loss: 0.6688 - val_accuracy: 0.5823\n",
      "Epoch 6/8\n",
      "105/105 [==============================] - 17s 166ms/step - loss: 0.6265 - accuracy: 0.6332 - val_loss: 0.6736 - val_accuracy: 0.5823\n",
      "Epoch 7/8\n",
      "105/105 [==============================] - 17s 166ms/step - loss: 0.6192 - accuracy: 0.6370 - val_loss: 0.6807 - val_accuracy: 0.5785\n",
      "Epoch 8/8\n",
      "105/105 [==============================] - 17s 165ms/step - loss: 0.6087 - accuracy: 0.6454 - val_loss: 0.6850 - val_accuracy: 0.5789\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f808c10fc70>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=EPOCHS, verbose=1,\n",
    "         validation_data=(X_test, y_test), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          [@elleasinswell, oh,, i'll, have, to, try, it!...\n",
       "1          [@cmlundy, done!!!!!, i, really, need, one, to...\n",
       "2          [lost, google, notebook, ie, add, on, with, th...\n",
       "3          [@natalietran, at, least, they, have, a, moral...\n",
       "4          [@kirstyhilton, ive, been, trying, to, get, mi...\n",
       "                                 ...                        \n",
       "1599995    [ain't, watching, the, laker, game,, i, can't,...\n",
       "1599996    [bummed, about, the, softball, loss, 0-1, thes...\n",
       "1599997    [back, in, god's, hands,, back, in, god's, han...\n",
       "1599998                                         [bbq, party]\n",
       "1599999    [@iamjemzie, what, time, is, this, and, where,...\n",
       "Name: text, Length: 1600000, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0.77\n",
    "ef strip(row):\n",
    "    return row.split()\n",
    "\n",
    "data['text'].apply(lambda x: strip(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "374"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'].str.len().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@cmlundy done!!!!! i really need one too... aritzia hasnt gotten back yet  dammit!!'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
