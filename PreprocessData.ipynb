{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e501560f-da90-4ba9-bc9d-8c4684cd76fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import tensorflow_text as tf_text\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense, InputLayer, GlobalMaxPool1D, Dropout, Conv1D, MaxPool1D, Flatten, Embedding, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.activations import relu, sigmoid\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "82b8a51a-3cc6-4bd3-b55a-c7f18cfce0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORD2VEC \n",
    "W2V_SIZE = 300\n",
    "W2V_WINDOW = 7\n",
    "W2V_EPOCH = 32\n",
    "W2V_MIN_COUNT = 10\n",
    "\n",
    "# KERAS \n",
    "SEQUENCE_LENGTH = 300\n",
    "EPOCHS = 8\n",
    "BATCH_SIZE = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6da2a9ae-d17d-403d-8d74-6778c6aece4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_data_prep():\n",
    "    data = pd.read_csv('Data/twitter_data.csv', \n",
    "                      encoding='latin-1', names=['sentiment', 'id', 'date', 'flag', 'user',\n",
    "                                                'text'])\n",
    "    data['sentiment'].replace(4, 1, inplace=True)\n",
    "    data = data.sample(frac=1).reset_index(drop=True).copy()\n",
    "    data['text'] = data['text'].str.lower()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bcd13b7-0843-4b32-ad1a-b160ac329811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_nicknames(row):\n",
    "    # Prepare list of words\n",
    "    words = row.split()\n",
    "    # Remove nicknames\n",
    "    for word in words:\n",
    "        if word[0] == '@':\n",
    "            words.remove(word)\n",
    "    # Return string \n",
    "    return ' '.join(word for word in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bdfc756-2ac8-49e5-8d79-f3c7387ba9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split(df, size):\n",
    "    # Split dataset into smaller one \n",
    "    col_list = list(df.columns)\n",
    "    # Drop target column name\n",
    "    col_list.pop(0)\n",
    "    x_train, x_valid = train_test_split(\n",
    "    df, random_state=1, stratify=df['sentiment'], test_size=size)\n",
    "    # Prepare new indexes \n",
    "    x_valid.reset_index(drop=True, inplace=True)\n",
    "    return x_valid \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cda9969-ca0d-43a6-9f89-1342dcde9332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_space(text):\n",
    "    # Replace new rows with space \n",
    "    text = text.replace('\\n', \" \").replace(\"\\r\", \" \")\n",
    "    # Create list of all not needed chars \n",
    "    punc_list = '!\"@#$%^&*()+_-.<>?/:;[]{}|\\~'\n",
    "    # Make transformation with dict that contains punc_list chars\n",
    "    t = str.maketrans(dict.fromkeys(punc_list, \" \"))\n",
    "    # Apply transformation\n",
    "    text = text.translate(t)\n",
    "    # Replace single quote with empty char\n",
    "    t = text.maketrans(dict.fromkeys(\"'`\"))\n",
    "    text.translate(t)\n",
    "    \n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc9cf7e8-aeb0-4998-b220-4ed427c348e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text):\n",
    "    # Prepare set of stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.remove('Not')\n",
    "\n",
    "    # Remove stopwords from the text\n",
    "    filtered_text = [word for word in text.split() if not word in stop_words]\n",
    "    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f7a94e9-484f-468d-9576-2f1a35aa86bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data, num_words, num_words_pad): \n",
    "    data = data.copy()\n",
    "    # Apply replace func that replace chars with spaces\n",
    "    data['text'] = data['text'].apply(lambda x: replace_with_space(x)).copy()\n",
    "    # Apply func that removes stop words\n",
    "    data['text'] = data['text'].apply(lambda x: remove_stop_words(x))\n",
    "    \n",
    "    \n",
    "    w2v_model = gensim.models.word2vec.Word2Vec(vector_size=W2V_SIZE, \n",
    "                                            window=W2V_WINDOW, \n",
    "                                            min_count=W2V_MIN_COUNT, \n",
    "                                            workers=8)\n",
    "    \n",
    "    documents = [_text.split() for _text in df_train.text] \n",
    "    w2v_model.build_vocab(documents)\n",
    "\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    tok = tf.keras.preprocessing.text.Tokenizer(num_words=num_words)\n",
    "    # Updates internal vocabulary based on a list of texts \n",
    "    tok.fit_on_texts(list(data['text']))\n",
    "    # Transforms each text in texts to a sequence of integers.\n",
    "    seq = tok.texts_to_sequences(list(data['text']))\n",
    "    # Pad sequences to make them same lenght \n",
    "    tf_ready = tf.keras.preprocessing.sequence.pad_sequences(seq)\n",
    "    \n",
    "    return tf_ready, tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2b76cd1-e239-460d-afc8-117376c6d559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-9c3bebceab38>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['text'] = data['text'].apply(lambda x: replace_with_space(x)).copy()\n",
      "<ipython-input-9-9c3bebceab38>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['text'] = data['text'].apply(lambda x: remove_stop_words(x))\n"
     ]
    }
   ],
   "source": [
    "data1 = first_data_prep()\n",
    "data = stratified_split(data1, 0.1)\n",
    "# Apply replace func that replace chars with spaces\n",
    "data['text'] = data['text'].apply(lambda x: replace_with_space(x)).copy()\n",
    "# Apply func that removes stop words\n",
    "data['text'] = data['text'].apply(lambda x: remove_stop_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b4e9805-4e45-4718-97bd-8480b90c108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.word2vec.Word2Vec(vector_size=W2V_SIZE, \n",
    "                                            window=W2V_WINDOW, \n",
    "                                            min_count=W2V_MIN_COUNT, \n",
    "                                            workers=8)\n",
    "    \n",
    "documents = data.text\n",
    "w2v_model.build_vocab(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e00fa934-b6fb-40e1-8d57-50eb98268783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 147682\n"
     ]
    }
   ],
   "source": [
    "words = w2v_model.wv.index_to_key\n",
    "vocab_size = len(tok.word_index) + 1\n",
    "print(\"Vocab size\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ed6087b-3742-4281-aa3b-6857d91518f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32087341, 41551488)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "427cb6db-41c4-412f-bade-a158536ce919",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = tf.keras.preprocessing.text.Tokenizer(num_words=100000)\n",
    "# Updates internal vocabulary based on a list of texts \n",
    "tok.fit_on_texts(list(data['text']))\n",
    "# Transforms each text in texts to a sequence of integers.\n",
    "seq = tok.texts_to_sequences(list(data['text']))\n",
    "# Pad sequences to make them same lenght \n",
    "tf_ready = tf.keras.preprocessing.sequence.pad_sequences(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "38a09d4c-33f1-40af-90fb-92fda1b2dc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(147682, 300)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, W2V_SIZE))\n",
    "for word, i in tok.word_index.items():\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[i] = w2v_model.wv[word]\n",
    "embedding_layer = Embedding(vocab_size, W2V_SIZE,\n",
    "                            weights=[embedding_matrix], input_length=SEQUENCE_LENGTH, trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3465d0-75c6-4d89-b009-152411685208",
   "metadata": {},
   "source": [
    "data = first_data_prep()\n",
    "split_data= stratified_split(data, 0.1)\n",
    "tf_ready, tok = tokenize(split_data, 10000, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "19f6f475-754a-4a5d-befa-afe190c7ca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_df = pd.DataFrame(tf_ready)\n",
    "tf_df['sentiment'] = data['sentiment']\n",
    "# tf_df.to_csv('tokenized_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9231c5e3-8e3a-4d12-82cc-77a522af74d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_cnn(data,\n",
    "                embedding,\n",
    "                layers=None, \n",
    "                dropout_rate=0,\n",
    "                kernel_size=10,\n",
    "                stride=10,\n",
    "                pool_size=2,\n",
    "                optimizer='Adam',\n",
    "                loss='binary_crossentropy',\n",
    "                 kernel_initializer='lecun_normal',\n",
    "                 kernel_regularizer=tf.keras.regularizers.L2(0.01)\n",
    "                ):\n",
    "    \"\"\" Layers argument shape:\n",
    "    [[number of nodes, activate function], \n",
    "    [number of nodes, activate function],\n",
    "    ...]\n",
    "    \n",
    "    \"\"\"\n",
    "    input_len = data.shape[1] - 1 \n",
    "    model = Sequential()\n",
    "    model.add(embedding)\n",
    "    model.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "    model.add(MaxPool1D(3))\n",
    "    model.add(Conv1D(64, 3, activation='relu'))\n",
    "    model.add(MaxPool1D(pool_size=3))\n",
    "    model.add(Conv1D(128, 3, activation='relu'))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    if dropout_rate > 0:\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    if layers != None:\n",
    "        for node in layers[1:]:\n",
    "            model.add(Dense(node[0], activation=node[1], kernel_initializer=kernel_initializer, \n",
    "                            kernel_regularizer=kernel_regularizer))\n",
    "    model.add(Dense(10, activation='sigmoid'))\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_initializer=kernel_initializer, \n",
    "                    kernel_regularizer=kernel_regularizer))\n",
    "    \n",
    "    model.compile(loss=loss, \n",
    "                 optimizer=optimizer,\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a7c6a7a4-0408-43f8-a578-f64a7596ef81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_rnn(data,\n",
    "                embedding,\n",
    "                layers=None, \n",
    "                dropout_rate=0,\n",
    "                kernel_size=10,\n",
    "                stride=10,\n",
    "                pool_size=2,\n",
    "                optimizer='Adam',\n",
    "                loss='binary_crossentropy',\n",
    "                 kernel_initializer='lecun_normal',\n",
    "                 kernel_regularizer=tf.keras.regularizers.L2(0.01)\n",
    "                ):\n",
    "    \"\"\" Layers argument shape:\n",
    "    [[number of nodes, activate function], \n",
    "    [number of nodes, activate function],\n",
    "    ...]\n",
    "    \n",
    "    \"\"\"\n",
    "    input_len = data.shape[1] - 1 \n",
    "    model = Sequential()\n",
    "    model.add(embedding)\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))    \n",
    "\n",
    "    if layers != None:\n",
    "        for node in layers[1:]:\n",
    "            model.add(Dense(node[0], activation=node[1], kernel_initializer=kernel_initializer, \n",
    "                            kernel_regularizer=kernel_regularizer))\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_initializer=kernel_initializer, \n",
    "                    kernel_regularizer=kernel_regularizer))\n",
    "    \n",
    "    model.compile(loss=loss, \n",
    "                 optimizer=optimizer,\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9a2dc25a-9ada-4e71-81f9-f0a95d56de7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = configure_cnn(tf_df, embedding_layer)\n",
    "model_rnn = configure_rnn(tf_df, embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c2648268-6ef0-42bd-b48d-e46301c18b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tf_df.columns.tolist()\n",
    "features.remove('sentiment')\n",
    "X_train, X_test, y_train, y_test = train_test_split(tf_df[features], tf_df['sentiment'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d4fc0a88-9209-4c53-b872-6f80843ded3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 300, 300)          44304600  \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 300, 32)           28832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 100, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 98, 64)            6208      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 32, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 30, 128)           24704     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 44,365,645\n",
      "Trainable params: 61,045\n",
      "Non-trainable params: 44,304,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_rnn.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "434b941d-575a-4cfe-be12-27df523677c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 300) for input KerasTensor(type_spec=TensorSpec(shape=(None, 300), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 49).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 300) for input KerasTensor(type_spec=TensorSpec(shape=(None, 300), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 49).\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.7205 - accuracy: 0.5246WARNING:tensorflow:Model was constructed with shape (None, 300) for input KerasTensor(type_spec=TensorSpec(shape=(None, 300), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 49).\n",
      "105/105 [==============================] - 14s 128ms/step - loss: 0.7202 - accuracy: 0.5248 - val_loss: 0.6702 - val_accuracy: 0.5859\n",
      "Epoch 2/8\n",
      "105/105 [==============================] - 16s 149ms/step - loss: 0.6631 - accuracy: 0.5951 - val_loss: 0.6654 - val_accuracy: 0.5868\n",
      "Epoch 3/8\n",
      "105/105 [==============================] - 17s 166ms/step - loss: 0.6525 - accuracy: 0.6064 - val_loss: 0.6645 - val_accuracy: 0.5761\n",
      "Epoch 4/8\n",
      "105/105 [==============================] - 18s 167ms/step - loss: 0.6463 - accuracy: 0.6084 - val_loss: 0.6653 - val_accuracy: 0.5850\n",
      "Epoch 5/8\n",
      "105/105 [==============================] - 17s 165ms/step - loss: 0.6360 - accuracy: 0.6207 - val_loss: 0.6688 - val_accuracy: 0.5823\n",
      "Epoch 6/8\n",
      "105/105 [==============================] - 17s 166ms/step - loss: 0.6265 - accuracy: 0.6332 - val_loss: 0.6736 - val_accuracy: 0.5823\n",
      "Epoch 7/8\n",
      "105/105 [==============================] - 17s 166ms/step - loss: 0.6192 - accuracy: 0.6370 - val_loss: 0.6807 - val_accuracy: 0.5785\n",
      "Epoch 8/8\n",
      "105/105 [==============================] - 17s 165ms/step - loss: 0.6087 - accuracy: 0.6454 - val_loss: 0.6850 - val_accuracy: 0.5789\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f808c10fc70>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=EPOCHS, verbose=1,\n",
    "         validation_data=(X_test, y_test), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "04965c36-95a7-46fd-a9cb-49d354337080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          [@elleasinswell, oh,, i'll, have, to, try, it!...\n",
       "1          [@cmlundy, done!!!!!, i, really, need, one, to...\n",
       "2          [lost, google, notebook, ie, add, on, with, th...\n",
       "3          [@natalietran, at, least, they, have, a, moral...\n",
       "4          [@kirstyhilton, ive, been, trying, to, get, mi...\n",
       "                                 ...                        \n",
       "1599995    [ain't, watching, the, laker, game,, i, can't,...\n",
       "1599996    [bummed, about, the, softball, loss, 0-1, thes...\n",
       "1599997    [back, in, god's, hands,, back, in, god's, han...\n",
       "1599998                                         [bbq, party]\n",
       "1599999    [@iamjemzie, what, time, is, this, and, where,...\n",
       "Name: text, Length: 1600000, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0.77\n",
    "ef strip(row):\n",
    "    return row.split()\n",
    "\n",
    "data['text'].apply(lambda x: strip(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c7dd931c-24e8-429f-9bb8-24554e091f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "374"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'].str.len().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "78053a6b-8142-4091-8562-019c4279c695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@cmlundy done!!!!! i really need one too... aritzia hasnt gotten back yet  dammit!!'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd00c7d-54f2-4c45-8e06-daeb4de9a4cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
